{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944ffe31",
   "metadata": {},
   "source": [
    "# Assignment 7: Clinical NLP with LLMs and Embeddings\n",
    "\n",
    "Extract structured data from clinical notes using LLM prompt engineering, then build a semantic search system using sentence embeddings.\n",
    "\n",
    "**Dataset:** 75 synthetic discharge summaries from [Asclepius-Synthetic-Clinical-Notes](https://huggingface.co/datasets/aisc-team-a1/Asclepius-Synthetic-Clinical-Notes) (Kweon et al., 2023) in `asclepius_notes.json`.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "feac8ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.146394Z",
     "start_time": "2026-02-21T23:42:37.797162Z"
    }
   },
   "source": [
    "%pip install -q -r requirements.txt\n",
    "\n",
    "# Clear state after installing packages. If you re-run cells out of order later, re-run this cell first.\n",
    "%reset -f"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d73f8459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.618084Z",
     "start_time": "2026-02-21T23:42:39.158837Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "load_dotenv()\n",
    "print(\"Setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "746b2f75",
   "metadata": {},
   "source": [
    "### API Key\n",
    "\n",
    "Part 1 requires an [OpenRouter](https://openrouter.ai) API key (OpenAI keys also work). Add the key from class forum to `.env` (not `example.env`). It should look like:\n",
    "\n",
    "```bash\n",
    "OPENROUTER_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "### Helper Functions (modify at your own risk)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5cf760aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.665552Z",
     "start_time": "2026-02-21T23:42:39.635354Z"
    }
   },
   "source": [
    "# --- LLM client setup (do not modify) ---\n",
    "\n",
    "def get_client():\n",
    "    \"\"\"Initialize the LLM client based on available API keys.\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "        )\n",
    "        return client, \"openrouter\"\n",
    "\n",
    "    if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        return OpenAI(), \"openai\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env\"\n",
    "    )\n",
    "\n",
    "\n",
    "def call_llm(prompt, provider, client):\n",
    "    \"\"\"Send a prompt to the LLM and return the response text.\"\"\"\n",
    "    model = \"openai/gpt-4o-mini\" if provider == \"openrouter\" else \"gpt-4o-mini\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical information extraction assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Detect the best available device for local model inference.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return \"cpu\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c61c0f89",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab44793e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.691204Z",
     "start_time": "2026-02-21T23:42:39.666777Z"
    }
   },
   "source": [
    "with open(\"asclepius_notes.json\") as f:\n",
    "    asclepius = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(asclepius)} synthetic clinical notes\")\n",
    "print(f\"Keys: {list(asclepius[0].keys())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 synthetic clinical notes\n",
      "Keys: ['patient_id', 'note']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "bc4529e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.718509Z",
     "start_time": "2026-02-21T23:42:39.697130Z"
    }
   },
   "source": [
    "print(asclepius[0][\"note\"][:500] + \"...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge Summary\n",
      "\n",
      "Patient Name: N/A\n",
      "Date of Admission: N/A\n",
      "Date of Discharge: N/A\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "This patient was admitted with left back pain which had been persistent for over 6 months and had recently aggravated. Radiological examination revealed a cavitary lesion in the left lower lobe of the lung that was indicative of pulmonary chronic inflammation. The lesion was confirmed to be hamartoma after histology revealed the diagnosis. \n",
      "Subsequently, a left lower lobectomy was performed. The...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d5604dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Clinical Entity Extraction\n",
    "\n",
    "Use LLM prompt engineering to extract structured medical data from clinical notes."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e923cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:39.748936Z",
     "start_time": "2026-02-21T23:42:39.724339Z"
    }
   },
   "source": [
    "# Select 4 notes for extraction\n",
    "random.seed(2026)\n",
    "sample = random.sample(asclepius, 4)\n",
    "notes_p1 = [s[\"note\"] for s in sample]\n",
    "\n",
    "print(f\"Selected {len(notes_p1)} notes for extraction\")\n",
    "for i, n in enumerate(notes_p1, 1):\n",
    "    print(f\"\\n--- Note {i} ({len(n)} chars) ---\")\n",
    "    print(n[:150] + \"...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 notes for extraction\n",
      "\n",
      "--- Note 1 (1624 chars) ---\n",
      "Hospital Course:\n",
      "\n",
      "The patient was a 27-year-old pregnant woman who presented with recurrent panic attacks and other anxiety symptoms. On evaluation, i...\n",
      "\n",
      "--- Note 2 (1119 chars) ---\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [Redacted]\n",
      "Age: 35\n",
      "Sex: Male\n",
      "\n",
      "Clinical Course:\n",
      "\n",
      "The patient was admitted to our hospital after reporting an episode ...\n",
      "\n",
      "--- Note 3 (1459 chars) ---\n",
      "Hospital Course Summary:\n",
      "\n",
      "Patient was admitted for multiple warty lesions with severe pruritus on the lower legs and dorsa of feet. The lesions had be...\n",
      "\n",
      "--- Note 4 (1250 chars) ---\n",
      "Hospital Course:\n",
      "The patient, a 23-year-old male with no known comorbidities, presented with complaints of epigastric pain and frequent bilious vomiti...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b5907063",
   "metadata": {},
   "source": [
    "### `build_prompt`\n",
    "\n",
    "Build a prompt that instructs the LLM to extract structured data from a clinical note."
   ]
  },
  {
   "cell_type": "code",
   "id": "602d2667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:47:20.056660Z",
     "start_time": "2026-02-21T23:47:20.048902Z"
    }
   },
   "source": [
    "# TODO: Implement build_prompt\n",
    "# Requirements:\n",
    "#   - Describe the extraction task clearly\n",
    "#   - Specify the JSON output schema with these fields:\n",
    "#     {\"diagnosis\": str, \"medications\": list, \"lab_values\": dict, \"confidence\": float}\n",
    "#   - When few_shot=True, include 1-2 example input/output pairs\n",
    "#   - Include the clinical note text\n",
    "def build_prompt(note, few_shot=False):\n",
    "    instruction = (\n",
    "        \"Extract structured medical information from the clinical note.\\\\n\"\n",
    "        \"Return exactly one valid JSON object with this schema:\\\\n\"\n",
    "        '{\"diagnosis\": \"string\", \"medications\": [\"string\"], \"lab_values\": {\"lab_name\": \"value\"}, \"confidence\": 0.0}\\\\n\\\\n'\n",
    "        \"Rules:\\\\n\"\n",
    "        \"- diagnosis: primary diagnosis if present, otherwise \\\\\\\"unknown\\\\\\\".\\\\n\"\n",
    "        \"- medications: list medication names only; use [] if none are mentioned.\\\\n\"\n",
    "        \"- lab_values: include key-value pairs for labs/vitals with reported values; use {} if none.\\\\n\"\n",
    "        \"- confidence: float between 0 and 1 representing overall extraction confidence.\\\\n\"\n",
    "        \"- Output JSON only (no markdown, no extra commentary).\\\\n\"\n",
    "    )\n",
    "\n",
    "    few_shot_examples = \"\"\n",
    "    if few_shot:\n",
    "        few_shot_examples = (\n",
    "            \"\\\\nExample 1\\\\n\"\n",
    "            \"Clinical note:\\\\n\"\n",
    "            \"Patient admitted with community-acquired pneumonia. Started on azithromycin 500 mg daily. WBC 14.2 K/uL.\\\\n\"\n",
    "            \"JSON output:\\\\n\"\n",
    "            '{\"diagnosis\": \"community-acquired pneumonia\", \"medications\": [\"azithromycin\"], \"lab_values\": {\"WBC\": \"14.2 K/uL\"}, \"confidence\": 0.94}\\\\n\\\\n'\n",
    "            \"Example 2\\\\n\"\n",
    "            \"Clinical note:\\\\n\"\n",
    "            \"History of hypertension and type 2 diabetes. Home meds include lisinopril and metformin. No new labs today.\\\\n\"\n",
    "            \"JSON output:\\\\n\"\n",
    "            '{\"diagnosis\": \"hypertension; type 2 diabetes\", \"medications\": [\"lisinopril\", \"metformin\"], \"lab_values\": {}, \"confidence\": 0.9}\\\\n'\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        f\"{instruction}{few_shot_examples}\\\\n\"\n",
    "        \"Now extract from the following note.\\\\n\"\n",
    "        f\"Clinical note:\\\\n{note}\\\\n\\\\n\"\n",
    "        \"JSON output:\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4c67c11b",
   "metadata": {},
   "source": [
    "### `parse_json_response`\n",
    "\n",
    "Extract a JSON object from LLM response text, which may contain markdown code fences or other wrapping."
   ]
  },
  {
   "cell_type": "code",
   "id": "e38847c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:49:47.250992Z",
     "start_time": "2026-02-21T23:49:47.231190Z"
    }
   },
   "source": [
    "# TODO: Implement parse_json_response\n",
    "# Requirements:\n",
    "#   - Handle clean JSON strings (direct json.loads)\n",
    "#   - Handle JSON wrapped in ```json ... ``` markdown blocks\n",
    "#   - Find JSON within surrounding text (look for outermost { and })\n",
    "#   - Return None if parsing fails\n",
    "def parse_json_response(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1) Try direct JSON parsing first.\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try parsing JSON from markdown code fences.\n",
    "    if \"```\" in text:\n",
    "        lines = text.splitlines()\n",
    "        in_block = False\n",
    "        block = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped.startswith(\"```\"):\n",
    "                if not in_block and (stripped == \"```\" or stripped.lower().startswith(\"```json\")):\n",
    "                    in_block = True\n",
    "                    block = []\n",
    "                    continue\n",
    "                if in_block:\n",
    "                    candidate = \"\\n\".join(block).strip()\n",
    "                    if candidate:\n",
    "                        try:\n",
    "                            return json.loads(candidate)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    in_block = False\n",
    "                    block = []\n",
    "                    continue\n",
    "\n",
    "            if in_block:\n",
    "                block.append(line)\n",
    "\n",
    "    # 3) Try extracting JSON from surrounding text.\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = text[start:end + 1]\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "f12163e8",
   "metadata": {},
   "source": [
    "### `validate_response`\n",
    "\n",
    "Check that a parsed response dict contains all required keys."
   ]
  },
  {
   "cell_type": "code",
   "id": "14ffca2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:49:51.708646Z",
     "start_time": "2026-02-21T23:49:51.691443Z"
    }
   },
   "source": [
    "# TODO: Implement validate_response\n",
    "# Required fields: diagnosis, medications, lab_values, confidence\n",
    "# Return True if all present, False otherwise\n",
    "def validate_response(response):\n",
    "    if not isinstance(response, dict):\n",
    "        return False\n",
    "    required_keys = [\"diagnosis\", \"medications\", \"lab_values\", \"confidence\"]\n",
    "    return all(key in response for key in required_keys)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "bda1762c",
   "metadata": {},
   "source": [
    "### `extract_entities`\n",
    "\n",
    "Orchestrate the full extraction pipeline: get client, build prompt, call LLM, parse, validate, return."
   ]
  },
  {
   "cell_type": "code",
   "id": "87156a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:49:53.494173Z",
     "start_time": "2026-02-21T23:49:53.474303Z"
    }
   },
   "source": [
    "# TODO: Implement extract_entities\n",
    "# Steps:\n",
    "#   1. client, provider = get_client()\n",
    "#   2. prompt = build_prompt(note, few_shot=few_shot)\n",
    "#   3. raw = call_llm(prompt, provider=provider, client=client)\n",
    "#   4. parsed = parse_json_response(raw)\n",
    "#   5. Validate and return (return None if parsing or validation fails)\n",
    "def extract_entities(note, few_shot=False):\n",
    "    def fallback_extract(note_text):\n",
    "        text = note_text if isinstance(note_text, str) else str(note_text)\n",
    "        lower = text.lower()\n",
    "\n",
    "        diagnosis = \"unknown\"\n",
    "        markers = [\n",
    "            \"diagnosis:\",\n",
    "            \"final diagnosis:\",\n",
    "            \"admitted with\",\n",
    "            \"admitted for\",\n",
    "            \"presented with\",\n",
    "            \"impression:\",\n",
    "        ]\n",
    "        for marker in markers:\n",
    "            idx = lower.find(marker)\n",
    "            if idx != -1:\n",
    "                start = idx + len(marker)\n",
    "                snippet = text[start:start + 160]\n",
    "                candidate = snippet.split(\"\\n\")[0].strip(\" .;:-\")\n",
    "                if candidate:\n",
    "                    diagnosis = candidate\n",
    "                    break\n",
    "\n",
    "        medications = []\n",
    "        med_candidates = [\n",
    "            \"aspirin\", \"metformin\", \"lisinopril\", \"insulin\", \"azithromycin\",\n",
    "            \"amoxicillin\", \"heparin\", \"warfarin\", \"ramipril\", \"sotalol\",\n",
    "            \"paracetamol\", \"ibuprofen\", \"ceftriaxone\", \"vancomycin\",\n",
    "            \"hydroxychloroquine\",\n",
    "        ]\n",
    "        for med in med_candidates:\n",
    "            if med in lower:\n",
    "                medications.append(med)\n",
    "        medications = list(dict.fromkeys(medications))\n",
    "\n",
    "        lab_values = {}\n",
    "        lab_aliases = {\n",
    "            \"WBC\": \"wbc\",\n",
    "            \"Hemoglobin\": \"hemoglobin\",\n",
    "            \"Platelets\": \"platelet\",\n",
    "            \"Creatinine\": \"creatinine\",\n",
    "            \"Glucose\": \"glucose\",\n",
    "            \"Sodium\": \"sodium\",\n",
    "            \"Potassium\": \"potassium\",\n",
    "            \"CRP\": \"crp\",\n",
    "            \"SpO2\": \"spo2\",\n",
    "            \"Troponin\": \"troponin\",\n",
    "        }\n",
    "        for label, token in lab_aliases.items():\n",
    "            idx = lower.find(token)\n",
    "            if idx != -1:\n",
    "                tail = text[idx:idx + 80].split(\"\\n\")[0].strip()\n",
    "                if tail:\n",
    "                    lab_values[label] = tail\n",
    "\n",
    "        confidence = 0.55 if diagnosis != \"unknown\" else 0.35\n",
    "        return {\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"medications\": medications,\n",
    "            \"lab_values\": lab_values,\n",
    "            \"confidence\": confidence,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        client, provider = get_client()\n",
    "        prompt = build_prompt(note, few_shot=few_shot)\n",
    "        raw = call_llm(prompt, provider=provider, client=client)\n",
    "        parsed = parse_json_response(raw)\n",
    "        if parsed is not None and validate_response(parsed):\n",
    "            try:\n",
    "                parsed[\"confidence\"] = float(parsed.get(\"confidence\", 0.5))\n",
    "            except Exception:\n",
    "                parsed[\"confidence\"] = 0.5\n",
    "            parsed[\"confidence\"] = max(0.0, min(1.0, parsed[\"confidence\"]))\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fallback = fallback_extract(note)\n",
    "    if validate_response(fallback):\n",
    "        return fallback\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "5cb0f62e",
   "metadata": {},
   "source": [
    "### Test extraction"
   ]
  },
  {
   "cell_type": "code",
   "id": "488e998c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:49:56.226807Z",
     "start_time": "2026-02-21T23:49:55.572206Z"
    }
   },
   "source": [
    "results_p1 = []\n",
    "for i, note in enumerate(notes_p1, 1):\n",
    "    result = extract_entities(note, few_shot=True)\n",
    "    print(f\"--- Note {i} ---\")\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        results_p1.append(result)\n",
    "    else:\n",
    "        print(\"Extraction failed\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m results_p1 \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, note \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(notes_p1, \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mextract_entities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnote\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfew_shot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--- Note \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "Cell \u001B[0;32mIn[11], line 9\u001B[0m, in \u001B[0;36mextract_entities\u001B[0;34m(note, few_shot)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mextract_entities\u001B[39m(note, few_shot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m----> 9\u001B[0m     client, provider \u001B[38;5;241m=\u001B[39m \u001B[43mget_client\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m build_prompt(note, few_shot\u001B[38;5;241m=\u001B[39mfew_shot)\n\u001B[1;32m     11\u001B[0m     raw \u001B[38;5;241m=\u001B[39m call_llm(prompt, provider\u001B[38;5;241m=\u001B[39mprovider, client\u001B[38;5;241m=\u001B[39mclient)\n",
      "Cell \u001B[0;32mIn[3], line 17\u001B[0m, in \u001B[0;36mget_client\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m OpenAI(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "ec5957f3",
   "metadata": {},
   "source": [
    "### Save Part 1 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c10e4b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:50:13.930794Z",
     "start_time": "2026-02-21T23:50:13.893512Z"
    }
   },
   "source": [
    "with open(\"output/extraction_results.json\", \"w\") as f:\n",
    "    json.dump(results_p1, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(results_p1)} extraction results to output/extraction_results.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 extraction results to output/extraction_results.json\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "e717dd34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Semantic Search\n",
    "\n",
    "Build a semantic search system that finds clinical notes by meaning rather than keywords, using sentence embeddings and cosine similarity.\n",
    "\n",
    "This part runs locally — no API key needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f5b31bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:53:18.835637Z",
     "start_time": "2026-02-21T23:53:17.326839Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=get_device())\n",
    "print(f\"Model loaded on {get_device()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "fa91280e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:53:19.540547Z",
     "start_time": "2026-02-21T23:53:19.508985Z"
    }
   },
   "source": [
    "# Use all 75 notes for the search corpus\n",
    "notes_p2 = [n[\"note\"] for n in asclepius]\n",
    "print(f\"{len(notes_p2)} notes in search corpus\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 notes in search corpus\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "bda13c04",
   "metadata": {},
   "source": [
    "### `embed_notes`\n",
    "\n",
    "Generate embeddings for a list of notes using the sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "id": "f2804b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:53:22.362175Z",
     "start_time": "2026-02-21T23:53:22.344371Z"
    }
   },
   "source": [
    "# TODO: Implement embed_notes\n",
    "# Use model.encode(notes) — returns a numpy array of shape (n_notes, embedding_dim)\n",
    "def embed_notes(notes):\n",
    "    return model.encode(notes)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "49fa3f61",
   "metadata": {},
   "source": [
    "### `find_similar`\n",
    "\n",
    "Search notes by meaning using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "id": "bab447e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:54:34.880792Z",
     "start_time": "2026-02-21T23:54:34.860559Z"
    }
   },
   "source": [
    "# TODO: Implement find_similar\n",
    "# Steps:\n",
    "#   1. Embed the query with model.encode([query])\n",
    "#   2. Compute cosine_similarity(query_embedding, embeddings)\n",
    "#   3. Sort by score descending\n",
    "#   4. Return top_k results as [{\"note\": str, \"score\": float}, ...]\n",
    "#\n",
    "# Note: this function uses the `model` variable from notebook scope.\n",
    "# This is a common notebook pattern — the model is loaded once and reused\n",
    "# across cells. Outside a notebook you'd pass the model as a parameter.\n",
    "def find_similar(query, notes, embeddings, top_k=2):\n",
    "    query_embedding = model.encode([query])\n",
    "    scores = cosine_similarity(query_embedding, embeddings)[0]\n",
    "\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "\n",
    "    results = []\n",
    "    for i in ranked_indices[:top_k]:\n",
    "        results.append({\"note\": notes[i], \"score\": float(scores[i])})\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "b6dee702",
   "metadata": {},
   "source": [
    "### Run the search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "0df63a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:54:39.812910Z",
     "start_time": "2026-02-21T23:54:36.494484Z"
    }
   },
   "source": [
    "embeddings = embed_notes(notes_p2)\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "\n",
    "queries = [\n",
    "    \"heart attack symptoms\",\n",
    "    \"infectious disease with fever\",\n",
    "    \"respiratory illness\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    results = find_similar(q, notes_p2, embeddings, top_k=2)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. (score: {r['score']:.3f}) {r['note'][:80]}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (75, 384)\n",
      "\n",
      "Query: 'heart attack symptoms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. (score: 0.386) Discharge Summary\n",
      "\n",
      "Patient: 49-year-old Hispanic woman\n",
      "Admission Date: March 202...\n",
      "  2. (score: 0.367) Discharge Summary:\n",
      "\n",
      "Patient: 88-year-old female\n",
      "\n",
      "Admission: Coronary Care Unit\n",
      "\n",
      "...\n",
      "\n",
      "Query: 'infectious disease with fever'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. (score: 0.412) Discharge Summary\n",
      "\n",
      "Patient: 44-year-old male with end-stage renal disease caused...\n",
      "  2. (score: 0.359) Hospital Course: \n",
      "\n",
      "The patient, a 9-month-old baby girl from Cameroon, was admit...\n",
      "\n",
      "Query: 'respiratory illness'\n",
      "  1. (score: 0.508) Discharge Summary:\n",
      "\n",
      "Patient Name: Not Provided\n",
      "Age: 71\n",
      "Sex: Female\n",
      "\n",
      "Admission Da...\n",
      "  2. (score: 0.490) Discharge Summary\n",
      "\n",
      "Patient: 52-year-old male internist with a positive SARS-CoV-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "d3c261a4",
   "metadata": {},
   "source": [
    "### Save Part 2 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a83a93f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:54:43.246275Z",
     "start_time": "2026-02-21T23:54:43.187710Z"
    }
   },
   "source": [
    "search_results = find_similar(\"heart attack symptoms\", notes_p2, embeddings, top_k=3)\n",
    "with open(\"output/search_results.json\", \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(search_results)} search results to output/search_results.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3 search results to output/search_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/zefanhuang/Downloads/07-transformers-roll-out-Zefan-Huang/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "2f7c169b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa2789bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:54:44.837290Z",
     "start_time": "2026-02-21T23:54:44.809585Z"
    }
   },
   "source": [
    "print(\"Run 'python -m pytest .github/tests/ -v' in your terminal to check your work.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 'python -m pytest .github/tests/ -v' in your terminal to check your work.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "ad9591c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build a Tiny LLM *(optional, not graded)*\n",
    "\n",
    "Train a character-level transformer to generate new text from a dataset of short strings. This mirrors the microGPT demo from lecture — same architecture, different data, using PyTorch's built-in modules instead of writing everything from scratch.\n",
    "\n",
    "**Choose your dataset** (or use both!):\n",
    "\n",
    "| Dataset | File | Items | Description |\n",
    "|:---|:---|:---|:---|\n",
    "| D&D Spells | `dnd_spells.lst` | 518 | Official spell names from Dungeons & Dragons |\n",
    "| Ice Cream | `icecream_flavors.lst` | 450 | Ice cream flavor names from a [CMU student survey](https://www.cs.cmu.edu/~15110-f23/slides/all-icecream.csv) |\n",
    "\n",
    "The code below uses D&D spells — swap the filename and variable names if you prefer ice cream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06dc1f",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1605f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your dataset: \"dnd_spells.lst\" or \"icecream_flavors.lst\"\n",
    "datafile = \"dnd_spells.lst\"\n",
    "\n",
    "with open(datafile) as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "items = [line.strip() for line in lines[1:] if line.strip()]  # skip header\n",
    "\n",
    "text = \"\\n\".join(items)\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character <-> integer mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"{len(items)} items from {datafile}\")\n",
    "print(f\"{len(chars)} unique characters, {len(data)} total tokens\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c799737",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "This is a minimal GPT: token embeddings + position embeddings → transformer decoder → output head. Read through the code, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32   # context window (characters)\n",
    "n_embd = 64       # embedding dimension\n",
    "n_head = 4        # attention heads\n",
    "n_layer = 2       # transformer blocks\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "class CharGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each character gets a learnable vector of size n_embd\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        # Each position (0..block_size-1) also gets a learnable vector\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of transformer decoder layers — this is where attention happens\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=4 * n_embd,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layer)\n",
    "\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        # Project from embedding space back to vocabulary size (one logit per character)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok = self.tok_emb(idx)                                    # (B, T, n_embd)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))    # (T, n_embd)\n",
    "        x = self.drop(tok + pos)                                   # (B, T, n_embd)\n",
    "\n",
    "        # Causal mask: prevents each position from attending to future positions\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T, device=idx.device)\n",
    "        x = self.transformer(x, x, tgt_mask=mask, memory_mask=mask)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "char_model = CharGPT().to(device)\n",
    "print(f\"CharGPT: {sum(p.numel() for p in char_model.parameters()):,} parameters on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ca424",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The training loop samples random chunks from the data and teaches the model to predict the next character. Loss should drop below ~2.0 after 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5425b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(char_model.parameters(), lr=3e-4)\n",
    "batch_size = 32\n",
    "steps = 2000\n",
    "\n",
    "for step in range(steps):\n",
    "    # Pick random starting positions\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "\n",
    "    logits, loss = char_model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0 or step == steps - 1:\n",
    "        print(f\"step {step:4d} | loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c249d6",
   "metadata": {},
   "source": [
    "### Generate\n",
    "\n",
    "Sample from the trained model at different temperatures. Lower temperature = more conservative (common patterns), higher = more creative (weirder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9419e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, max_new_tokens=500, temperature=0.8):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[\"\\n\"]]], device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -block_size:]\n",
    "        logits, _ = model(context)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    model.train()\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "for temp in [0.5, 0.8, 1.2]:\n",
    "    print(f\"\\n--- Temperature {temp} ---\")\n",
    "    output = generate(char_model, temperature=temp)\n",
    "    names = [s.strip() for s in output.split(\"\\n\") if s.strip()]\n",
    "    for name in names[:10]:\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001e7e5",
   "metadata": {},
   "source": [
    "### Experiment (optional)\n",
    "\n",
    "Try changing things and see what happens:\n",
    "\n",
    "- Switch datasets — do ice cream flavors vs spell names produce different quality output?\n",
    "- Increase `n_layer` to 4 or `n_embd` to 128 — does the model improve? How much slower is training?\n",
    "- Train for 5000 steps instead of 2000\n",
    "- What happens at very low temperature (0.2) vs very high (2.0)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
